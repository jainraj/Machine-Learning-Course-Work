import pandas
from tqdm import tqdm
import re
from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk import pos_tag
from nltk.corpus import wordnet, stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.utils.fixes import loguniform
from time import time
import string

seed = 0  # This does not set seed. We will use this value as and when required
n_jobs = -3  # number of cores to use

base_path = '/home/raj/Desktop/CourseWork/E0-270/Project/SNLI/data/snli_1.0_'
train_data = pandas.read_csv(base_path + 'train.csv', usecols=['sentence1_binary_parse', 'sentence2_binary_parse', 'gold_label'])

non_word_chars = set(string.ascii_lowercase) - {'a', 'i'}
tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
lemmatizer = WordNetLemmatizer()
stop_words_list = list(stopwords.words("english"))


def make_tokens(sentence):
    tokens = sentence.replace('(', '').replace(')', '').split()  # Get the tokens generated by the stanford parser
    tokens = map(lambda x: x.lower(), tokens)  # take lower because capital letters are not going to affect the inference
    tokens = filter(lambda x: re.search('[a-zA-Z]', x), tokens)  # remove those tokens which are purely punctuation marks or numbers
    tokens = filter(lambda x: x not in non_word_chars, tokens)  # remove characters which are not words in their own right
    tokens = list(tokens)  # perform the previous operations on demand
    pos_tags = pos_tag(tokens)  # tag the tokens
    pos_tags = map(lambda pos_tag: tag_dict.get(pos_tag[1][0].upper(), wordnet.NOUN), pos_tags)  # convert pos tags to a format which the lemmatizer can use
    tokens = map(lambda token, pos_tag: lemmatizer.lemmatize(token, pos_tag), tokens, pos_tags)  # lemmatize the word with its pos tag
    tokens = list(tokens)  # perform the previous operations on demand
    return tokens if tokens else None  # if the sentence ends up giving no useful tokens, remove it from data


def preprocess_data(df):
    df = df.copy()  # do not change the data in global sense
    df.dropna(inplace=True)  # remove any datapoints which has missing data in the columns which are required for us
    df = df[df.gold_label != '-'].copy()  # Remove data which does not have gold label
    tqdm.pandas(desc='Making tokens')
    df['premise_tokens'] = df.sentence1_binary_parse.progress_apply(make_tokens)
    df['hypothesis_tokens'] = df.sentence2_binary_parse.progress_apply(make_tokens)
    df.dropna(inplace=True)  # remove the sentences if no valid tokens were found
    return df[['premise_tokens', 'hypothesis_tokens', 'gold_label']]


def get_grids():  # We want some complex combinations which scikit doesn't support yet.
    stop_words = [None, stop_words_list]
    use_idfs = [True, False]
    solvers_penalties_Cs = {
        'lbfgs': {
            'l2': loguniform(1, 1e3),
            'none': [1],  # C will be ignored
        },
        'saga': {
            'l2': loguniform(1, 1e3),
            'none': [1],  # C will be ignored
        },
    }

    grids = []
    for stop_word in stop_words:
        for use_idf in use_idfs:
            for solver, penalties_Cs in solvers_penalties_Cs.items():
                for penalty, Cs in penalties_Cs.items():
                    grids.append({
                        'processor__premise_processor__stop_words': [stop_word],
                        'processor__hypothesis_processor__stop_words': [stop_word],
                        'processor__premise_processor__use_idf': [use_idf],
                        'processor__hypothesis_processor__use_idf': [use_idf],
                        'lr__solver': [solver],
                        'lr__penalty': [penalty],
                        'lr__C': Cs,
                    })

    return grids


def identity(x): return x


features_processor = ColumnTransformer(
    transformers=[
        ('premise_processor', TfidfVectorizer(preprocessor=identity, tokenizer=identity), 'premise_tokens'),
        ('hypothesis_processor', TfidfVectorizer(preprocessor=identity, tokenizer=identity), 'hypothesis_tokens')
    ],
    sparse_threshold=0.99,
)

pipeline = Pipeline([
    ('processor', features_processor),
    ('lr', LogisticRegression(multi_class='multinomial', class_weight='balanced', random_state=seed, n_jobs=n_jobs)),
], verbose=True)

clf = RandomizedSearchCV(
    estimator=pipeline,
    param_distributions=get_grids(),
    n_iter=200,
    cv=StratifiedKFold(n_splits=4, shuffle=True, random_state=seed),
    n_jobs=n_jobs,
    random_state=seed,
    return_train_score=False,
    refit=False,
    verbose=10,
)

processed_train_data = preprocess_data(train_data)

clf.fit(processed_train_data, processed_train_data.gold_label)
df = pandas.DataFrame(clf.cv_results_)
df.to_pickle('cv_results.pkl')
