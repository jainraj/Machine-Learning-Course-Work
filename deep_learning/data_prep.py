import re
import os
import pandas
import string
from tqdm import tqdm
from nltk import pos_tag
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

base_path = '/home/raj/Desktop/CourseWork/E0-270/Project/SNLI/'
non_word_chars = set(string.ascii_lowercase) - {'a', 'i'}
tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
lemmatizer = WordNetLemmatizer()
label_map = {
    'neutral': 0,
    'entailment': 1,
    'contradiction': 2,
}


def make_tokens(sentence):
    tokens = sentence.replace('(', '').replace(')', '').split()  # Get the tokens generated by the stanford parser
    tokens = map(lambda x: x.lower(), tokens)  # take lower because capital letters are not going to affect the inference
    tokens = filter(lambda x: re.search('[a-zA-Z]', x), tokens)  # remove those tokens which are purely punctuation marks or numbers
    tokens = filter(lambda x: x not in non_word_chars, tokens)  # remove characters which are not words in their own right
    tokens = list(tokens)  # perform the previous operations on demand
    pos_tags = pos_tag(tokens)  # tag the tokens
    pos_tags = map(lambda pos_tag: tag_dict.get(pos_tag[1][0].upper(), wordnet.NOUN), pos_tags)  # convert pos tags to a format which the lemmatizer can use
    tokens = map(lambda token, pos_tag: lemmatizer.lemmatize(token, pos_tag), tokens, pos_tags)  # lemmatize the word with its pos tag
    tokens = list(tokens)  # perform the previous operations on demand
    return tokens if tokens else None  # if the sentence ends up giving no useful tokens, remove it from data


def preprocess_data(df):
    df = df.copy()  # do not change the data in global sense
    df['label'] = df.gold_label.apply(label_map.get)
    df.dropna(inplace=True)  # remove any datapoints which has missing data in the columns which are required for us & those which does not have a gold label
    tqdm.pandas(desc='Making tokens')
    df['premise_tokens'] = df.sentence1_binary_parse.progress_apply(make_tokens)
    df['hypothesis_tokens'] = df.sentence2_binary_parse.progress_apply(make_tokens)
    df.dropna(inplace=True)  # remove the sentences if no valid tokens were found
    return df[['premise_tokens', 'hypothesis_tokens', 'label']]


def get_preprocessed_data():
    data_path = base_path + 'data/'
    if os.path.isfile(data_path + 'train_data.pkl') and os.path.isfile(data_path + 'test_data.pkl'):
        processed_train_data = pandas.read_pickle(data_path + 'train_data.pkl')
        processed_test_data = pandas.read_pickle(data_path + 'test_data.pkl')
    else:
        train_data = pandas.read_csv(data_path + 'snli_1.0_train.csv',
                                     usecols=['sentence1_binary_parse', 'sentence2_binary_parse', 'gold_label'])
        test_data = pandas.read_csv(data_path + 'snli_1.0_test.csv',
                                    usecols=['sentence1_binary_parse', 'sentence2_binary_parse', 'gold_label'])
        processed_train_data = preprocess_data(train_data)
        processed_test_data = preprocess_data(test_data)
        processed_train_data.to_pickle(data_path + 'train_data.pkl')
        processed_test_data.to_pickle(data_path + 'test_data.pkl')
    return processed_train_data, processed_test_data


if __name__ == "__main__":
    get_preprocessed_data()
